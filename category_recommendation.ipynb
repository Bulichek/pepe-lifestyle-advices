{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "category_recommendation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuFnchSfGdd8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79b53ec2-1b17-4e38-fd03-fc924527c400"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNkDouT4CfMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "! pip install --upgrade numpy pandas tqdm torch catalyst==20.09\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o08k0IFOCfM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from bisect import bisect_left, bisect_right\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "unk_token = \"<UNK>\"\n",
        "\n",
        "# GPU hack if you need\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD0TKDyMCfM7",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "\n",
        "Columns\n",
        "- `party_rk` – client unique identifier\n",
        "- `account_rk` – client account unique identifier\n",
        "- `financial_account_type_cd` – debit/credit card flag\n",
        "- `transaction_dttm` – operation datetime\n",
        "- `transaction_type_desc` – purchase/payment/...\n",
        "- `transaction_amt_rur` – transaction price\n",
        "- `merchant_type` - DUTY FREE STORES/FUEL DEALERS/RESTAURANTS/ etc\n",
        "- `merchant_group_rk` - McDonald's/Wildberries/ etc\n",
        "\n",
        "It's important that table is already sorted by `transaction_dttm` column!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBPaDLAZDcIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Load and unpack data\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def load_from_gdrive(gid, filename='temp'):\n",
        "    !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$gid -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$gid -O $filename && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# !rm -rf data*\n",
        "load_from_gdrive('1-DNWpDk1RnMHaiEnbHVpv_vo9XBbO0iA', 'data.zip')\n",
        "!unzip data.zip -d /content/data/\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE1Zg4iXnhTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATADIR = \"/content/data\" # \"./data\"\n",
        "\n",
        "transactions_path = f\"{DATADIR}/avk_hackathon_data_transactions.csv\"\n",
        "socdem_path = f\"{DATADIR}/avk_hackathon_data_party_x_socdem.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mU2gKJIWUeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUILD_MAPPINGS = False\n",
        "BUILD_PATRY_DATA = False\n",
        "\n",
        "TARGET_VARIABLE = 'category' # `merchant_type` or `category`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT0HZR2_CfNA",
        "colab_type": "text"
      },
      "source": [
        "## Mappings\n",
        "~1 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKHwvxB5CfNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Prepare & save mappings\n",
        "\n",
        "def create_mapping(values):\n",
        "    mapping = {unk_token: 0}\n",
        "    for v in values:\n",
        "        if not pd.isna(v) and v != unk_token:\n",
        "            mapping[str(v)] = len(mapping)\n",
        "\n",
        "    return mapping\n",
        "\n",
        "if BUILD_MAPPINGS:\n",
        "    mappings = defaultdict(dict)\n",
        "\n",
        "    for col in tqdm(\n",
        "        [\n",
        "            \"transaction_type_desc\",\n",
        "            \"merchant_rk\",\n",
        "            \"merchant_type\",\n",
        "            \"merchant_group_rk\",\n",
        "            \"category\",\n",
        "            \"financial_account_type_cd\",\n",
        "        ]\n",
        "    ):\n",
        "\n",
        "        col_values = (\n",
        "            pd.read_csv(transactions_path, usecols=[col])[col]\n",
        "            .fillna(unk_token)\n",
        "            .astype(str)\n",
        "        )\n",
        "        mappings[col] = create_mapping(col_values.unique())\n",
        "        del col_values\n",
        "\n",
        "    for col in tqdm(\n",
        "        [\n",
        "            \"gender_cd\",\n",
        "            \"age\",\n",
        "            \"marital_status_desc\",\n",
        "            \"children_cnt\",\n",
        "            \"region_flg\",\n",
        "        ]\n",
        "    ):\n",
        "\n",
        "        col_values = (\n",
        "            pd.read_csv(socdem_path, usecols=[col])[col]\n",
        "            .fillna(unk_token)\n",
        "            .astype(str)\n",
        "        )\n",
        "        mappings[col] = create_mapping(col_values.unique())\n",
        "        del col_values\n",
        "\n",
        "\n",
        "    with open(f\"{DATADIR}/mappings.json\", \"w\") as f:\n",
        "        json.dump(mappings, f)\n",
        "\n",
        "else:\n",
        "    # load mappings\n",
        "    with open(f\"{DATADIR}/mappings.json\", 'r') as f:\n",
        "        mappings = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc69QUTfCfNG",
        "colab_type": "text"
      },
      "source": [
        "## Parse transactions by users\n",
        "~ 20 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaBYqe1RCxZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if BUILD_PATRY_DATA:\n",
        "    usecols = [\n",
        "        \"party_rk\",\n",
        "        \"transaction_dttm\",\n",
        "        \"transaction_amt_rur\",\n",
        "        \"merchant_type\",\n",
        "        \"transaction_type_desc\",\n",
        "        \"financial_account_type_cd\",\n",
        "        \"category\",\n",
        "    ]\n",
        "\n",
        "    str_cols = [\n",
        "        \"transaction_dttm\",\n",
        "        \"merchant_type\",\n",
        "        \"transaction_type_desc\",\n",
        "        \"financial_account_type_cd\",\n",
        "        \"category\",\n",
        "    ]\n",
        "\n",
        "    df = pd.read_csv(transactions_path, usecols=usecols)\n",
        "    df[\"transaction_amt_rur\"] = df[\"transaction_amt_rur\"].fillna(0)\n",
        "    df[str_cols] = df[str_cols].fillna(unk_token).astype(str)\n",
        "    # df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opxFdcq5Dhcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if BUILD_PATRY_DATA:\n",
        "    party_list = df.party_rk.unique()\n",
        "\n",
        "    # Prepare & save client data\n",
        "    party2dates = dict()  # for each party save a series of the transaction dates \n",
        "    party2sum = dict()  # for each party save a series of the transaction costs \n",
        "    party2merchant_type = dict()  # for each party save a series of the transaction_type \n",
        "    party2trans_type = dict()  # for each party save a series of the transaction merchant_type\n",
        "    party2category = dict()\n",
        "    party2fin_acc_type = dict()\n",
        "\n",
        "    for party_rk in tqdm(party_list):\n",
        "        party_rows = df[df.party_rk == party_rk]\n",
        "        party_rows = party_rows.sort_values('transaction_dttm')\n",
        "\n",
        "        party2dates[party_rk] = list(party_rows.transaction_dttm.values)\n",
        "        party2sum[party_rk] = list(party_rows.transaction_amt_rur.values)\n",
        "        party2merchant_type[party_rk] = list(party_rows.merchant_type.values)\n",
        "        party2trans_type[party_rk] = list(party_rows.transaction_type_desc.values)\n",
        "        party2category[party_rk] = list(party_rows.category.values)\n",
        "        party2fin_acc_type[party_rk] = list(party_rows.financial_account_type_cd.values)\n",
        "        \n",
        "\n",
        "    # map values\n",
        "    party2merchant_type = {x[0]: [mappings[\"merchant_type\"][y] for y in x[1]] for x in party2merchant_type.items()}\n",
        "    party2trans_type = {x[0]: [mappings[\"transaction_type_desc\"][y] for y in x[1]] for x in party2trans_type.items()}\n",
        "    party2category = {x[0]: [mappings[\"category\"][y] for y in x[1]] for x in party2category.items()}\n",
        "    party2fin_acc_type = {x[0]: [mappings[\"financial_account_type_cd\"][y] for y in x[1]] for x in party2fin_acc_type.items()}\n",
        "\n",
        "    # add month and week days\n",
        "    dates = pd.date_range(\"2019-01-01\", \"2020-02-28\").tolist()#.strftime(\"%Y-%m-%d\").tolist()\n",
        "    weekdays = {date.strftime(\"%Y-%m-%d\"): date.weekday() for date in dates}\n",
        "    monthdays = {date.strftime(\"%Y-%m-%d\"): date.day - 1 for date in dates}\n",
        "\n",
        "    def add_weekdays():\n",
        "        party2weekday = dict()\n",
        "        party2day = dict()\n",
        "        party_list = pd.read_csv(transactions_path, usecols=['party_rk']).party_rk.unique()\n",
        "\n",
        "        for party_rk in party_list:\n",
        "            party2weekday[party_rk] = [weekdays[date] for date in party2dates[party_rk]]\n",
        "            party2day[party_rk] = [monthdays[date] for date in party2dates[party_rk]]\n",
        "\n",
        "        return party2weekday, party2day\n",
        "\n",
        "    party2weekday, party2day = add_weekdays()\n",
        "\n",
        "    # add user profiles\n",
        "    def get_user_profiles():\n",
        "        party2user = dict()\n",
        "        party_table = pd.read_csv(socdem_path).fillna(unk_token).astype(str)\n",
        "\n",
        "        for index, row in party_table.iterrows():\n",
        "            party2user[int(row.party_rk)] = {\n",
        "                col: mappings[col][row[col]]\n",
        "                for col in [\"gender_cd\", \"age\", \"marital_status_desc\", \"children_cnt\", \"region_flg\"]\n",
        "            }\n",
        "\n",
        "        return party2user\n",
        "\n",
        "    party2user = get_user_profiles()\n",
        "\n",
        "    # save data\n",
        "    party_data = {\n",
        "        'dates': party2dates,\n",
        "        'sum': party2sum,\n",
        "        'merchant_type': party2merchant_type,\n",
        "        'trans_type': party2trans_type,\n",
        "        'category': party2category,\n",
        "        'fin_acc_type': party2fin_acc_type,\n",
        "        'day': party2day,\n",
        "        'weekday': party2weekday,\n",
        "        'user': party2user,\n",
        "    }\n",
        "\n",
        "    for name, data in party_data.items():\n",
        "        data_x = {str(x[0]): x[1] for x in data.items()}\n",
        "\n",
        "        with open(f\"{DATADIR}/party2{name}.json\", \"w\") as f:\n",
        "            json.dump(data_x, f)\n",
        "\n",
        "else:\n",
        "    # load data\n",
        "    party_data = {\n",
        "        'dates': None,\n",
        "        'sum': None,\n",
        "        'merchant_type': None,\n",
        "        'trans_type': None,\n",
        "        'category': None,\n",
        "        'fin_acc_type': None,\n",
        "        'day': None,\n",
        "        'weekday': None,\n",
        "        'user': None,\n",
        "    }\n",
        "\n",
        "    for name in party_data.keys():\n",
        "        with open(f\"{DATADIR}/party2{name}.json\", \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        data = {int(x[0]): x[1] for x in data.items()}\n",
        "        party_data[name] = data\n",
        "\n",
        "    party_data['day'] = {key: [v - 1 for v in values] for key, values in party_data['day'].items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtLQTSfDCfNL",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf2Pmq0FCfNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c3b09f9-3046-4cf5-f3ac-102239538e73"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_party, valid_party = train_test_split(\n",
        "    pd.read_csv(transactions_path, usecols=['party_rk']).party_rk.unique(), \n",
        "    train_size=0.8, random_state=42\n",
        ")\n",
        "# train_party = pd.read_csv(transactions_path, usecols=['party_rk']).party_rk.unique()\n",
        "\n",
        "print(f'Train: {len(train_party)} Val: {len(valid_party)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 40000 Val: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CGZuEgEoCfNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_period_len = 60  # -- days\n",
        "train_predict_dates = (\n",
        "    # pd.date_range(\"2019-03-01\", \"2019-10-31\", freq=\"MS\")\n",
        "    pd.date_range(\"2019-03-01\", \"2019-12-31\", freq=\"MS\")\n",
        "    .strftime(\"%Y-%m-%d\")\n",
        "    .tolist()\n",
        ")\n",
        "valid_predict_dates = (\n",
        "    pd.date_range(\"2019-11-01\", \"2019-12-31\", freq=\"MS\")\n",
        "    .strftime(\"%Y-%m-%d\")\n",
        "    .tolist()\n",
        ")\n",
        "submission_predict_dates = (\n",
        "    pd.date_range(\"2020-01-01\", \"2020-02-28\", freq=\"2MS\")\n",
        "    .strftime(\"%Y-%m-%d\")\n",
        "    .tolist()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6rAD6SdCfNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(party_list, mode=\"train\", target=TARGET_VARIABLE):\n",
        "    \"\"\"\n",
        "    This function define the pipeline of the creation of train and valid samples.\n",
        "    We consider each client from party_list. For each client take each \n",
        "    predict_period_start from predict_dates list. All client transaction before\n",
        "    this date is our features. Next, we look at the customer's transactions in \n",
        "    the next two months. This transactions should be predicted. It will form \n",
        "    our labels vector.\n",
        "    \"\"\"\n",
        "\n",
        "    data = {\n",
        "        'user': [],\n",
        "        'sum': [],\n",
        "        'trans_type': [],\n",
        "        'merchant_type': [],\n",
        "        'category': [],\n",
        "        'fin_acc_type': [],\n",
        "        'day': [],\n",
        "        'weekday': [],\n",
        "        'labels_merchant_type': [],\n",
        "        'labels_category': [],\n",
        "    }\n",
        "\n",
        "    for party_rk in tqdm(party_list):\n",
        "        user_info = party_data['user'][party_rk]\n",
        "        date_series = party_data['dates'][party_rk]\n",
        "        sum_series = party_data['sum'][party_rk]\n",
        "        merch_type_series = party_data['merchant_type'][party_rk]\n",
        "        trans_type_series = party_data['trans_type'][party_rk]\n",
        "        category_series = party_data['category'][party_rk]\n",
        "        fin_acc_type_series = party_data['fin_acc_type'][party_rk]\n",
        "        day_series = party_data['day'][party_rk]\n",
        "        weekday_series = party_data['weekday'][party_rk]\n",
        "\n",
        "        if mode == \"train\":\n",
        "            predict_dates = train_predict_dates\n",
        "        elif mode == \"valid\":\n",
        "            predict_dates = valid_predict_dates\n",
        "        elif mode == \"submission\":\n",
        "            predict_dates = submission_predict_dates\n",
        "        else:\n",
        "            raise Exception(\"Unknown mode\")\n",
        "\n",
        "        for predict_period_start in predict_dates:\n",
        "\n",
        "            predict_period_end = datetime.strftime(\n",
        "                datetime.strptime(predict_period_start, \"%Y-%m-%d\")\n",
        "                + timedelta(days=predict_period_len),\n",
        "                \"%Y-%m-%d\",\n",
        "            )\n",
        "\n",
        "            l, r = (\n",
        "                bisect_left(date_series, predict_period_start),\n",
        "                bisect_right(date_series, predict_period_end),\n",
        "            )\n",
        "\n",
        "            history_merch_type = merch_type_series[:l]\n",
        "            history_sum = sum_series[:l]\n",
        "            history_trans_type = trans_type_series[:l]\n",
        "            history_category = category_series[:l]\n",
        "            history_fin_ccc_type = fin_acc_type_series[:l]\n",
        "            history_day = day_series[:l]\n",
        "            history_weekday = weekday_series[:l]\n",
        "            \n",
        "            predict_merch = merch_type_series[l:r]\n",
        "            predict_category = category_series[l:r]\n",
        "            non_empty = any(predict_merch) if target == 'merchant_type' else any(predict_category)\n",
        "\n",
        "            if non_empty and l or mode not in (\"train\", \"valid\"):\n",
        "                data['user'].append(user_info)\n",
        "                data['sum'].append(history_sum)\n",
        "                data['trans_type'].append(history_trans_type)\n",
        "                data['merchant_type'].append(history_merch_type)\n",
        "                data['category'].append(history_category)\n",
        "                data['fin_acc_type'].append(history_fin_ccc_type)\n",
        "                data['merchant_type'].append(history_merch_type)\n",
        "                data['day'].append(history_day)\n",
        "                data['weekday'].append(history_weekday)\n",
        "                data['labels_merchant_type'].append(predict_merch)\n",
        "                data['labels_category'].append(predict_category)\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZzpIPNNCfNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f06a38-3a76-4486-8c39-d43fc8c586e3"
      },
      "source": [
        "train_data = prepare_data(train_party, mode=\"train\")\n",
        "valid_data = prepare_data(valid_party, mode=\"valid\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [00:25<00:00, 1543.02it/s]\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 10764.87it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-xdScilCfNW",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gUqipggCfNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex8SKiPqCfNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MERCH_TYPE_NCLASSES = len(mappings['merchant_type'])\n",
        "TRANS_TYPE_NCLASSES = len(mappings['transaction_type_desc'])\n",
        "SIZES = dict(map(lambda x: (x[0], len(x[1])), mappings.items()))\n",
        "\n",
        "SIZES['trans_type'] = 5\n",
        "SIZES['fin_acc_type'] = 3\n",
        "SIZES['day'] = 31\n",
        "SIZES['weekday'] = 7\n",
        "\n",
        "PADDING_LEN = 256"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjllhPWGCfNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RSDataset(Dataset):\n",
        "    def __init__(self, data, target=TARGET_VARIABLE):\n",
        "        super(RSDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        assert self.target in ('merchant_type', 'category')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['sum'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        targets = np.zeros((SIZES[self.target] - 1,), dtype=np.float32)\n",
        "        for m in self.data[f'labels_{self.target}'][idx]:\n",
        "            if m:  # skip UNK, UNK-token should not be predicted\n",
        "                targets[m - 1] = 1.0\n",
        "\n",
        "        item = {\n",
        "            \"features\": {},\n",
        "            \"targets\": targets,\n",
        "        }\n",
        "\n",
        "        sum_feature = np.array(self.data['sum'][idx][-PADDING_LEN:])\n",
        "        sum_feature = np.vectorize(lambda s: np.log(1 + s))(sum_feature)\n",
        "        if sum_feature.shape[0] < PADDING_LEN:\n",
        "            pad = np.zeros(\n",
        "                (PADDING_LEN - sum_feature.shape[0],), dtype=np.float32\n",
        "            )\n",
        "            sum_feature = np.hstack((sum_feature, pad))\n",
        "        item[\"features\"][\"sum\"] = torch.from_numpy(sum_feature).float()\n",
        "\n",
        "        for feature_name in [\"trans_type\", \"merchant_type\", \"category\", \"fin_acc_type\", \"day\", \"weekday\"]:\n",
        "            feature_values = self.data[feature_name][idx]\n",
        "            feature_values = np.array(feature_values[-PADDING_LEN:])\n",
        "            mask = np.ones(feature_values.shape[0], dtype=np.float32)\n",
        "            if feature_values.shape[0] < PADDING_LEN:\n",
        "                feature_values = np.append(\n",
        "                    feature_values,\n",
        "                    np.zeros(\n",
        "                        PADDING_LEN - feature_values.shape[0], dtype=np.int64\n",
        "                    ),\n",
        "                )\n",
        "                mask = np.append(\n",
        "                    mask,\n",
        "                    np.zeros(PADDING_LEN - mask.shape[0], dtype=np.float32),\n",
        "                )\n",
        "            item[\"features\"][feature_name] = torch.from_numpy(feature_values).long()\n",
        "        \n",
        "        item[\"features\"][\"mask\"] = torch.from_numpy(mask).float()\n",
        "\n",
        "        for feature_name in [\"gender_cd\", \"age\", \"marital_status_desc\", \"children_cnt\", \"region_flg\"]:\n",
        "            item[\"features\"][feature_name] = torch.tensor(self.data['user'][idx][feature_name]).long()\n",
        "\n",
        "        return item"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3-aV7lJCfNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = RSDataset(train_data, target=TARGET_VARIABLE)\n",
        "valid_dataset = RSDataset(valid_data, target=TARGET_VARIABLE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8jnB23BCfNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True, num_workers=2\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=64, shuffle=False, num_workers=2\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SGRUJWR3CfNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sanity check\n",
        "# for i in tqdm(range(len(train_loader))):\n",
        "#     batch = next(iter(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J67Ms2_jCfNk",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwwVKAh3AUx8",
        "colab_type": "text"
      },
      "source": [
        "This is the baseline model for predicting purchases in `merchant_type` in the next 2 months"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kWquDGFCfNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-PAEoqoCfNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    'emb_dim': {\n",
        "        'merchant_type': 87,\n",
        "        'category': 16,\n",
        "        'trans_type': 3,\n",
        "        'fin_acc_type': 1,\n",
        "        'day': 5,\n",
        "        'weekday': 3,\n",
        "        'gender_cd': 1,\n",
        "        'age': 4,\n",
        "        'marital_status_desc': 3,\n",
        "        'children_cnt': 3,\n",
        "        'region_flg': 1\n",
        "    },\n",
        "    'transformer_nhead': 4,\n",
        "    'transformer_dim_feedforward': 256,\n",
        "    'transformer_dropout': 0.1,\n",
        "    'dense_unit': 256,\n",
        "    'num_layers': 3,\n",
        "}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeAfj7h_CfNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "045beda6-4d72-419e-a8a6-ef26419546bd"
      },
      "source": [
        "MERCH_TYPE_NCLASSES, TRANS_TYPE_NCLASSES\n",
        "sum(params['emb_dim'].values())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDhr4svJCfNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_layers = nn.ModuleDict({\n",
        "            name: nn.Embedding(SIZES[name], params[\"emb_dim\"][name])\n",
        "            for name in params[\"emb_dim\"]\n",
        "        })\n",
        "\n",
        "        embedding_size = sum(params['emb_dim'].values()) + 1\n",
        "\n",
        "        transformer_blocks = []\n",
        "        for i in range(params[\"num_layers\"]):\n",
        "            transformer_block = nn.TransformerEncoderLayer(\n",
        "                d_model=embedding_size,\n",
        "                nhead=params[\"transformer_nhead\"],\n",
        "                dim_feedforward=params[\"transformer_dim_feedforward\"],\n",
        "                dropout=params[\"transformer_dropout\"],\n",
        "            )\n",
        "            transformer_blocks.append(\n",
        "                (f\"transformer_block_{i}\", transformer_block)\n",
        "            )\n",
        "\n",
        "        self.transformer_encoder = nn.Sequential(\n",
        "            OrderedDict(transformer_blocks)\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=embedding_size, out_features=params[\"dense_unit\"]\n",
        "        )\n",
        "        self.scorer = nn.Linear(\n",
        "            in_features=params[\"dense_unit\"],\n",
        "            out_features=SIZES[TARGET_VARIABLE] - 1,\n",
        "        )\n",
        "\n",
        "    def forward(self, features):\n",
        "        seq_embs = [\n",
        "            self.embedding_layers[feature_name](features[feature_name]) * features[\"mask\"].unsqueeze(-1)\n",
        "            # for feature_name in [\"merchant_type\", \"trans_type\", \"fin_acc_type\", \"day\", \"weekday\"]\n",
        "            for feature_name in [\"merchant_type\", \"trans_type\", \"category\", \"fin_acc_type\", \"day\", \"weekday\"]\n",
        "        ]\n",
        "        seq_embs.append(features[\"sum\"].unsqueeze(-1))\n",
        "\n",
        "        user_embs = [\n",
        "            self.embedding_layers[feature_name](features[feature_name].view(-1, 1)) * features[\"mask\"].unsqueeze(-1)\n",
        "            for feature_name in [\"gender_cd\", \"age\", \"marital_status_desc\", \"children_cnt\", \"region_flg\"]\n",
        "        ]\n",
        "        embeddings = torch.cat(seq_embs + user_embs, dim=-1)\n",
        "\n",
        "        transformer_output = self.transformer_encoder(embeddings)\n",
        "\n",
        "        input_mask = features[\"mask\"]\n",
        "        lengths = input_mask.sum(dim=1).unsqueeze(1)\n",
        "        x = transformer_output * input_mask.unsqueeze(-1)\n",
        "        pooling = torch.sum(x, dim=1) / lengths\n",
        "\n",
        "        # pooling = torch.mean(transformer_output, dim=1)\n",
        "\n",
        "        linear = torch.tanh(self.linear(pooling))\n",
        "        merch_logits = self.scorer(linear)\n",
        "\n",
        "        return merch_logits"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NszoLAhbCfNu",
        "colab_type": "text"
      },
      "source": [
        "### One-batch-check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVdmq86KCfNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf8d1b76-3f5b-4f88-d077-40b3b4b81512"
      },
      "source": [
        "model = Model()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "batch = next(iter(train_loader))\n",
        "with torch.no_grad():\n",
        "    output = model(batch['features'])\n",
        "    loss = criterion(output, batch['targets'])\n",
        "print(loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7177)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhFyoW9hafXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # sanity check\n",
        "# for i in tqdm(range(len(train_loader))):\n",
        "#     batch = next(iter(train_loader))\n",
        "#     with torch.no_grad():\n",
        "#         output = model(batch['features'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZz71tEDCfNw",
        "colab_type": "text"
      },
      "source": [
        "## Train loop with [Catalyst](https://github.com/catalyst-team/catalyst)\n",
        "\n",
        "[A comprehensive step-by-step guide to basic and advanced features](https://github.com/catalyst-team/catalyst#step-by-step-guide).\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcF4frWZCfNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catalyst import dl, utils\n",
        "from catalyst.utils import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6EBSFvEwpQl",
        "colab_type": "text"
      },
      "source": [
        "## Custom metrics for this hackathon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OhGkjT0CfNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List, Optional, Sequence, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from catalyst.utils.metrics.functional import preprocess_multi_label_metrics\n",
        "from catalyst.utils.torch import get_activation_fn\n",
        "\n",
        "\n",
        "def multi_label_metrics(\n",
        "    outputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    threshold: Union[float, torch.Tensor],\n",
        "    activation: Optional[str] = None,\n",
        "    eps: float = 1e-7,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Computes multi-label precision for the specified activation and threshold.\n",
        "\n",
        "    Args:\n",
        "        outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
        "            indicates the probability of the example belonging to each of\n",
        "            the K classes, according to the model.\n",
        "        targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
        "            classes are associated with the N-th input\n",
        "            (eg: a row [0, 1, 0, 1] indicates that the example is\n",
        "            associated with classes 2 and 4)\n",
        "        threshold (float): threshold for for model output\n",
        "        activation (str): activation to use for model output\n",
        "        eps (float): epsilon to avoid zero division\n",
        "    \n",
        "    Extended version of \n",
        "        https://github.com/catalyst-team/catalyst/blob/master/catalyst/utils/metrics/accuracy.py#L58\n",
        "\n",
        "    Returns:\n",
        "        computed multi-label metrics\n",
        "    \"\"\"\n",
        "    outputs, targets, _ = preprocess_multi_label_metrics(\n",
        "        outputs=outputs, targets=targets\n",
        "    )\n",
        "    activation_fn = get_activation_fn(activation)\n",
        "    outputs = activation_fn(outputs)\n",
        "\n",
        "    outputs = (outputs > threshold).long()\n",
        "\n",
        "    accuracy = (targets.long() == outputs.long()).sum().float() / np.prod(\n",
        "        targets.shape\n",
        "    )\n",
        "\n",
        "    intersection = (outputs.long() * targets.long()).sum(axis=1).float()\n",
        "    num_predicted = outputs.long().sum(axis=1).float()\n",
        "    num_relevant = targets.long().sum(axis=1).float()\n",
        "    union = num_predicted + num_relevant\n",
        "\n",
        "    # Precision = ({predicted items} && {relevant items}) / {predicted items}\n",
        "    precision = intersection / (num_predicted + eps * (num_predicted == 0))\n",
        "    # Recall = ({predicted items} && {relevant items}) / {relevant items}\n",
        "    recall = intersection / (num_relevant + eps * (num_relevant == 0))\n",
        "    # IoU = ({predicted items} && {relevant items}) / ({predicted items} || {relevant items})\n",
        "    iou = (intersection + eps * (union == 0)) / (union - intersection + eps)\n",
        "\n",
        "    return accuracy, precision.mean(), recall.mean(), iou.mean()\n",
        "\n",
        "\n",
        "def precision_at_k(\n",
        "    actual: torch.Tensor, \n",
        "    predicted: torch.Tensor, \n",
        "    k: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes precision at cutoff k for one sample\n",
        "\n",
        "    Args:\n",
        "       actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
        "       predicted (torch.Tensor): binary tensor that encodes which of the K\n",
        "           classes are associated with the N-th input\n",
        "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
        "           associated with classes 2 and 4)\n",
        "       k (int): parameter k of precison@k\n",
        "\n",
        "    Returns:\n",
        "       Computed value of precision@k for given sample\n",
        "    \"\"\"\n",
        "    p_at_k = 0.0\n",
        "    for item in predicted[:k]:\n",
        "        if actual[item]:\n",
        "            p_at_k += 1\n",
        "    p_at_k /= k\n",
        "\n",
        "    return p_at_k\n",
        "\n",
        "\n",
        "def average_precision_at_k(\n",
        "    actual: torch.Tensor, \n",
        "    predicted: torch.Tensor, \n",
        "    k: int,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes average precision at cutoff k for one sample\n",
        "\n",
        "    Args:\n",
        "      actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
        "      predicted (torch.Tensor): binary tensor that encodes which of the K\n",
        "          classes are associated with the N-th input\n",
        "          (eg: a row [0, 1, 0, 1] indicates that the example is\n",
        "          associated with classes 2 and 4)\n",
        "      k (int): parameter k of AP@k\n",
        "\n",
        "    Returns:\n",
        "        Computed value of AP@k for given sample\n",
        "    \"\"\"\n",
        "    ap_at_k = 0.0\n",
        "    for idx, item in enumerate(predicted[:k]):\n",
        "        if actual[item]:\n",
        "            ap_at_k += precision_at_k(actual, predicted, k=idx + 1)\n",
        "    ap_at_k /= min(k, actual.sum().cpu().numpy())\n",
        "    \n",
        "\n",
        "    return ap_at_k\n",
        "\n",
        "\n",
        "def mean_average_precision_at_k(\n",
        "    output: torch.Tensor, target: torch.Tensor, top_k: Tuple[int, ...] = (1,)\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Computes mean_average_precision_at_k at set of cutoff parameters K\n",
        "\n",
        "    Args:\n",
        "       outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
        "           indicates the probability of the example belonging to each of\n",
        "           the K classes, according to the model.\n",
        "       targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
        "           classes are associated with the N-th input\n",
        "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
        "           associated with classes 2 and 4)\n",
        "       top_k (tuple): list of parameters k at which map@k will be computed\n",
        "\n",
        "\n",
        "    Returns:\n",
        "       List of computed values of map@k at each cutoff k from topk\n",
        "    \"\"\"\n",
        "    max_k = max(top_k)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, top_indices = output.topk(k=max_k, dim=1, largest=True, sorted=True)\n",
        "\n",
        "    result = []\n",
        "    for k in top_k:  # loop over k\n",
        "        map_at_k = 0.0\n",
        "        for actual_target, predicted_items in zip(\n",
        "            target, top_indices\n",
        "        ):  # loop over samples\n",
        "            map_at_k += average_precision_at_k(\n",
        "                actual_target, predicted_items, k\n",
        "            )\n",
        "        map_at_k = map_at_k / batch_size\n",
        "        result.append(map_at_k)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GygG0d1WCfN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What is Runner?\n",
        "# https://catalyst-team.github.io/catalyst/api/core.html#runner\n",
        "class CustomRunner(dl.Runner):\n",
        "\n",
        "    def _handle_batch(self, batch):\n",
        "        # model train/valid step\n",
        "        features, targets = batch[\"features\"], batch[\"targets\"]\n",
        "        logits = self.model(features)\n",
        "        scores = torch.sigmoid(logits)\n",
        "\n",
        "        loss = self.criterion(logits, targets)\n",
        "        accuracy, precision, recall, iou = multi_label_metrics(\n",
        "            logits, targets, threshold=0.5, activation=\"Sigmoid\"\n",
        "        )\n",
        "        map05, map10 = mean_average_precision_at_k(\n",
        "            scores, targets, top_k=(5, 10)\n",
        "        )\n",
        "        batch_metrics = {\n",
        "            \"loss\": loss,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"iou\": iou,\n",
        "            \"map05\": map05,\n",
        "            \"map10\": map10,\n",
        "            # \"map20\": map20,\n",
        "            # \"map30\": map30\n",
        "        }\n",
        "        \n",
        "        self.input = {\"features\": features, \"targets\": targets}\n",
        "        self.output = {\"logits\": logits, \"scores\": scores}\n",
        "        self.batch_metrics.update(batch_metrics)\n",
        "\n",
        "        if self.is_train_loader:\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "    \n",
        "    def predict_batch(self, batch):\n",
        "        # model inference step\n",
        "        batch = utils.maybe_recursive_call(batch, \"to\", device=self.device)\n",
        "        logits = self.model(batch[\"features\"])\n",
        "        scores = torch.sigmoid(logits)\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noTAPVPSCfN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "loaders = {\"train\": train_loader, \"valid\": valid_loader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvDep0BRCD5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -rf logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH-bnRkkSRYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir ./logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liazMpxFoZ9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "35b65321-89b3-4f3b-d693-50e135ac62d5"
      },
      "source": [
        "# For other minimal examples, please follow the link below\n",
        "# https://github.com/catalyst-team/catalyst#minimal-examples\n",
        "runner = CustomRunner()\n",
        "# model training\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=None,\n",
        "    loaders=loaders,\n",
        "    logdir=\"./logs\",\n",
        "    num_epochs=10,\n",
        "    verbose=True,\n",
        "    load_best_on_end=True,\n",
        "    overfit=False,  #  <<<--- DO NOT FORGET TO MAKE IT ``False`` \n",
        "                    #  (``True`` uses only one batch to check pipeline correctness)\n",
        "    callbacks=[\n",
        "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n",
        "        # dl.AveragePrecisionCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"ap\"),\n",
        "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
        "        # dl.AUCCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"auc\"),\n",
        "    ],\n",
        "    main_metric=\"iou\", # \"ap/mean\", \n",
        "    minimize_metric=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/10 * Epoch (train): 100% 5841/5841 [19:56<00:00,  4.88it/s, accuracy=0.944, iou=0.500, loss=0.174, map05=0.750, map10=0.875, precision=1.000, recall=0.500]\n",
            "1/10 * Epoch (valid): 100% 255/255 [00:41<00:00,  6.17it/s, accuracy=0.778, iou=0.318, loss=0.486, map05=0.667, map10=0.704, precision=0.667, recall=0.318]\n",
            "[2020-09-20 00:17:28,561] \n",
            "1/10 * Epoch 1 (train): accuracy=0.8689 | iou=0.4376 | loss=0.3118 | map05=0.7585 | map10=0.7303 | precision=0.7458 | recall=0.5580\n",
            "1/10 * Epoch 1 (valid): accuracy=0.8770 | iou=0.4599 | loss=0.2936 | map05=0.7783 | map10=0.7503 | precision=0.7214 | recall=0.6106\n",
            "2/10 * Epoch (train): 100% 5841/5841 [20:05<00:00,  4.85it/s, accuracy=0.889, iou=0.500, loss=0.284, map05=0.543, map10=0.597, precision=0.800, recall=0.571]\n",
            "2/10 * Epoch (valid): 100% 255/255 [00:41<00:00,  6.16it/s, accuracy=0.778, iou=0.348, loss=0.383, map05=0.667, map10=0.700, precision=0.571, recall=0.382]\n",
            "[2020-09-20 00:38:15,286] \n",
            "2/10 * Epoch 2 (train): accuracy=0.8761 | iou=0.4628 | loss=0.2965 | map05=0.7810 | map10=0.7536 | precision=0.7564 | recall=0.5848\n",
            "2/10 * Epoch 2 (valid): accuracy=0.8801 | iou=0.4667 | loss=0.2858 | map05=0.7807 | map10=0.7556 | precision=0.7265 | recall=0.6148\n",
            "3/10 * Epoch (train): 100% 5841/5841 [20:05<00:00,  4.85it/s, accuracy=0.861, iou=0.444, loss=0.331, map05=0.550, map10=0.577, precision=0.667, recall=0.571]\n",
            "3/10 * Epoch (valid): 100% 255/255 [00:41<00:00,  6.21it/s, accuracy=0.769, iou=0.333, loss=0.375, map05=0.667, map10=0.708, precision=0.568, recall=0.365]\n",
            "[2020-09-20 00:59:01,512] \n",
            "3/10 * Epoch 3 (train): accuracy=0.8779 | iou=0.4690 | loss=0.2924 | map05=0.7833 | map10=0.7571 | precision=0.7575 | recall=0.5910\n",
            "3/10 * Epoch 3 (valid): accuracy=0.8806 | iou=0.4743 | loss=0.2839 | map05=0.7860 | map10=0.7591 | precision=0.7229 | recall=0.6310\n",
            "4/10 * Epoch (train): 100% 5841/5841 [20:04<00:00,  4.85it/s, accuracy=0.722, iou=0.231, loss=0.624, map05=0.400, map10=0.405, precision=0.300, recall=0.500]\n",
            "4/10 * Epoch (valid): 100% 255/255 [00:41<00:00,  6.17it/s, accuracy=0.796, iou=0.371, loss=0.366, map05=0.667, map10=0.667, precision=0.597, recall=0.397]\n",
            "[2020-09-20 01:19:46,935] \n",
            "4/10 * Epoch 4 (train): accuracy=0.8789 | iou=0.4723 | loss=0.2900 | map05=0.7848 | map10=0.7591 | precision=0.7587 | recall=0.5937\n",
            "4/10 * Epoch 4 (valid): accuracy=0.8811 | iou=0.4817 | loss=0.2829 | map05=0.7818 | map10=0.7564 | precision=0.7099 | recall=0.6474\n",
            "5/10 * Epoch (train): 100% 5841/5841 [20:00<00:00,  4.87it/s, accuracy=0.722, iou=0.231, loss=0.548, map05=0.760, map10=0.717, precision=0.750, recall=0.250]\n",
            "5/10 * Epoch (valid): 100% 255/255 [00:40<00:00,  6.25it/s, accuracy=0.824, iou=0.420, loss=0.342, map05=0.667, map10=0.708, precision=0.586, recall=0.461]\n",
            "[2020-09-20 01:40:27,895] \n",
            "5/10 * Epoch 5 (train): accuracy=0.8795 | iou=0.4743 | loss=0.2884 | map05=0.7857 | map10=0.7601 | precision=0.7593 | recall=0.5952\n",
            "5/10 * Epoch 5 (valid): accuracy=0.8826 | iou=0.4820 | loss=0.2792 | map05=0.7845 | map10=0.7616 | precision=0.7267 | recall=0.6345\n",
            "6/10 * Epoch (train): 100% 5841/5841 [19:50<00:00,  4.91it/s, accuracy=0.972, iou=0.500, loss=0.165, map05=1.000, map10=1.000, precision=0.500, recall=1.000]\n",
            "6/10 * Epoch (valid): 100% 255/255 [00:40<00:00,  6.26it/s, accuracy=0.806, iou=0.397, loss=0.341, map05=0.667, map10=0.722, precision=0.565, recall=0.444]\n",
            "[2020-09-20 02:00:59,204] \n",
            "6/10 * Epoch 6 (train): accuracy=0.8801 | iou=0.4765 | loss=0.2871 | map05=0.7865 | map10=0.7610 | precision=0.7604 | recall=0.5968\n",
            "6/10 * Epoch 6 (valid): accuracy=0.8837 | iou=0.4803 | loss=0.2771 | map05=0.7880 | map10=0.7648 | precision=0.7416 | recall=0.6171\n",
            "7/10 * Epoch (train): 100% 5841/5841 [19:42<00:00,  4.94it/s, accuracy=0.778, iou=0.556, loss=0.700, map05=1.000, map10=0.890, precision=0.833, recall=0.625]\n",
            "7/10 * Epoch (valid): 100% 255/255 [00:41<00:00,  6.16it/s, accuracy=0.806, iou=0.383, loss=0.360, map05=0.667, map10=0.704, precision=0.600, recall=0.411]\n",
            "[2020-09-20 02:21:23,211] \n",
            "7/10 * Epoch 7 (train): accuracy=0.8804 | iou=0.4772 | loss=0.2863 | map05=0.7870 | map10=0.7615 | precision=0.7607 | recall=0.5970\n",
            "7/10 * Epoch 7 (valid): accuracy=0.8838 | iou=0.4812 | loss=0.2772 | map05=0.7885 | map10=0.7650 | precision=0.7301 | recall=0.6305\n",
            "8/10 * Epoch (train): 100% 5841/5841 [19:54<00:00,  4.89it/s, accuracy=0.694, iou=0.154, loss=0.547, map05=0.710, map10=0.583, precision=0.667, recall=0.167]\n",
            "8/10 * Epoch (valid): 100% 255/255 [00:40<00:00,  6.23it/s, accuracy=0.787, iou=0.369, loss=0.378, map05=0.667, map10=0.700, precision=0.597, recall=0.395]\n",
            "[2020-09-20 02:41:59,002] \n",
            "8/10 * Epoch 8 (train): accuracy=0.8809 | iou=0.4789 | loss=0.2854 | map05=0.7874 | map10=0.7619 | precision=0.7616 | recall=0.5978\n",
            "8/10 * Epoch 8 (valid): accuracy=0.8832 | iou=0.4849 | loss=0.2780 | map05=0.7863 | map10=0.7647 | precision=0.7376 | recall=0.6295\n",
            "9/10 * Epoch (train): 100% 5841/5841 [20:05<00:00,  4.85it/s, accuracy=0.944, iou=0.333, loss=0.116, map05=0.750, map10=0.750, precision=0.500, recall=0.500]\n",
            "9/10 * Epoch (valid): 100% 255/255 [00:41<00:00,  6.20it/s, accuracy=0.787, iou=0.349, loss=0.360, map05=0.733, map10=0.733, precision=0.613, recall=0.365]\n",
            "[2020-09-20 03:02:45,515] \n",
            "9/10 * Epoch 9 (train): accuracy=0.8812 | iou=0.4799 | loss=0.2846 | map05=0.7878 | map10=0.7624 | precision=0.7620 | recall=0.5987\n",
            "9/10 * Epoch 9 (valid): accuracy=0.8837 | iou=0.4741 | loss=0.2768 | map05=0.7852 | map10=0.7613 | precision=0.7520 | recall=0.6015\n",
            "10/10 * Epoch (train): 100% 5841/5841 [20:11<00:00,  4.82it/s, accuracy=0.861, iou=0.444, loss=0.329, map05=0.710, map10=0.714, precision=0.800, recall=0.500]\n",
            "10/10 * Epoch (valid): 100% 255/255 [00:41<00:00,  6.17it/s, accuracy=0.815, iou=0.392, loss=0.354, map05=0.667, map10=0.714, precision=0.618, recall=0.411]\n",
            "[2020-09-20 03:23:38,545] \n",
            "10/10 * Epoch 10 (train): accuracy=0.8815 | iou=0.4806 | loss=0.2839 | map05=0.7879 | map10=0.7626 | precision=0.7623 | recall=0.5990\n",
            "10/10 * Epoch 10 (valid): accuracy=0.8844 | iou=0.4761 | loss=0.2757 | map05=0.7860 | map10=0.7630 | precision=0.7560 | recall=0.5995\n",
            "Top best models:\n",
            "logs/checkpoints/train.8.pth\t0.4849\n",
            "=> Loading checkpoint logs/checkpoints/best_full.pth\n",
            "loaded state checkpoint logs/checkpoints/best_full.pth (global epoch 8, epoch 8, stage train)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQxuxPukCfN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model inference example\n",
        "# for prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n",
        "#     assert prediction.detach().cpu().numpy().shape[-1] == MERCH_TYPE_NCLASSES-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IFV8lYM9CfN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import plotly  # required for contrib\n",
        "# from catalyst.contrib.utils import plot_tensorboard_log\n",
        "\n",
        "# plot_tensorboard_log(logdir=\"./logs\", step=\"batch\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Bte9iiW0CfOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly  # required for contrib\n",
        "from catalyst.contrib.utils import plot_tensorboard_log\n",
        "\n",
        "plot_tensorboard_log(\n",
        "    logdir=\"./logs\", \n",
        "    step=\"epoch\", \n",
        "    metrics=[\n",
        "        \"loss\", \"accuracy\", \"precision\", \"recall\", \"iou\", \n",
        "        \"map05\", \"map10\", \"map20\",\n",
        "        \"ap/mean\", \"auc/mean\"\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOBi1DqJPt_F",
        "colab_type": "text"
      },
      "source": [
        "## Your task\n",
        "\n",
        "We suggest you to improve this baseline. Feel free to use any kind of model architectures, loss functions, inputs, etc. in your experiments.\n",
        "\n",
        "\n",
        "YOUR TASK is to predict purchases in `merchant_type` in **January-February 2020** for all the clients (50k) from the given dataset.\n",
        "\n",
        "SUBMISSION FORMAT: You should submit a `.csv` file in the following format. \n",
        "\n",
        "The submission file should contain two columns:\n",
        "* `party_rk` -- client unique identifier\n",
        "* `recommendations` -- list of the **top 30** predicted `merchant_type`, sorted by predicted proba (pay attention!) **separated by commas**. \n",
        "\n",
        "The `.csv` file separator should be **semicolon (\";\")**. The submission file example can be generated by the pipeline shown below.\n",
        "\n",
        "EVALUATION: Your submission will be evaluated by metric **MAP@30**. Scores for this part of the hackathon will be given according to the value of this metric.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-30pWSiQL7vY",
        "colab_type": "text"
      },
      "source": [
        "## Submission file example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylgn_EIXMQrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac9b5532-ee1f-499a-b233-6506d16285c3"
      },
      "source": [
        "# create data loader for submission\n",
        "full_party = pd.read_csv(transactions_path, usecols=['party_rk']).party_rk.unique()\n",
        "full_data = prepare_data(\n",
        "    full_party, mode=\"submission\"\n",
        ")\n",
        "full_dataset = RSDataset(\n",
        "   full_data\n",
        ")\n",
        "full_loader = DataLoader(\n",
        "    full_dataset, batch_size=64, shuffle=False, num_workers=8, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:07<00:00, 6754.24it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tdEPJ0gMahr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a699826-cfc0-4d59-9187-cfd224d107fd"
      },
      "source": [
        "# get predictions from the model\n",
        "predictions = []\n",
        "predictions_scores = []\n",
        "\n",
        "for scores in tqdm(runner.predict_loader(loader=full_loader), total = len(full_loader)):\n",
        "    _, top_indices = scores.topk(k=30, dim=1, largest=True, sorted=True)\n",
        "    top_indices += 1\n",
        "    predictions += top_indices.detach().cpu().tolist()\n",
        "    predictions_scores += scores.detach().cpu().tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [00:39<00:00, 19.99it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZhfVRYmMpdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # inverse mapping for merchant_type in predictions\n",
        "# category_inverse_mapping = {k: v for v, k in mappings['category'].items()}\n",
        "# def inverse_mapping(x):\n",
        "#     return list(map(category_inverse_mapping.get, x))\n",
        "\n",
        "# predictions = list(map(inverse_mapping, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_CjmgByL9Vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create submission table\n",
        "submission = pd.DataFrame({\n",
        "    \"party_rk\" : full_party, \n",
        "    \"recommendations\" : predictions\n",
        "})\n",
        "submission['recommendations'] = submission['recommendations'].apply(lambda x: \",\".join(map(str, x)))\n",
        "\n",
        "\n",
        "submission.to_csv('submission_PEPEtoners_categories_ids.csv', index=False, sep=\";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8qEWStAjkrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create submission table\n",
        "submission = pd.DataFrame({\n",
        "    \"party_rk\" : full_party, \n",
        "    \"recommendations\" : predictions_scores\n",
        "})\n",
        "submission['recommendations'] = submission['recommendations'].apply(lambda x: \",\".join(map(lambda y: str(round(y, 4)), x)))\n",
        "\n",
        "\n",
        "submission.to_csv('submission_PEPEtoners_categories_scores.csv', index=False, sep=\";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlaXnPY6pV2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}